{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KidoQJbn7i3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transcription and MP3 to MP4**"
      ],
      "metadata": {
        "id": "r1YIatYh7ntM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SpeechRecognition pydub\n",
        "!apt-get install ffmpeg -y"
      ],
      "metadata": {
        "id": "0yEgx4_V7sdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "# Directorio donde están los videos\n",
        "# Change with the path of the videos (Train, Test, Val)\n",
        "video_dir = \"/content/drive/MyDrive/ElderReact_Data/ElderReact_train\"\n",
        "output_dir = \"/content/drive/MyDrive/EderReact/ElderReact_train\"\n",
        "\n",
        "# Crear el directorio de salida si no existe\n",
        "os.makedirs(output_dir\n",
        "            , exist_ok=True)\n",
        "\n",
        "# Inicializar el reconocedor de voz\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Procesar cada archivo de video en el directorio\n",
        "for video_file in os.listdir(video_dir):\n",
        "    if video_file.endswith(\".mp4\"):  # Verificar que sea un archivo .mp4\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        audio_output = os.path.join(output_dir, f\"{os.path.splitext(video_file)[0]}.wav\")\n",
        "        text_output = os.path.join(output_dir, f\"{os.path.splitext(video_file)[0]}.txt\")\n",
        "\n",
        "        # Convertir video a audio\n",
        "        video = AudioSegment.from_file(video_path, format=\"mp4\")\n",
        "        audio = video.set_channels(1).set_frame_rate(16000).set_sample_width(2)\n",
        "        audio.export(audio_output, format=\"wav\")\n",
        "\n",
        "        # Transcribir el audio\n",
        "        with sr.AudioFile(audio_output) as source:\n",
        "            audio_text = r.record(source)\n",
        "\n",
        "        try:\n",
        "            # Reconocimiento de texto\n",
        "            text = r.recognize_google(audio_text, language='en-US')\n",
        "\n",
        "            # Guardar la transcripción en un archivo de texto\n",
        "            with open(text_output, \"w\") as file:\n",
        "                file.write(text)\n",
        "\n",
        "            print(f\"Transcripción guardada en: {text_output}\")\n",
        "        except sr.UnknownValueError:\n",
        "            print(f\"No se pudo transcribir el audio de: {video_file}\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Error con el servicio de reconocimiento de voz: {e}\")\n",
        "\n",
        "print(\"¡Transcripción completada para todos los videos!\")"
      ],
      "metadata": {
        "id": "UwzWABUC71eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter and save in individual files"
      ],
      "metadata": {
        "id": "QOhFyNdp89VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#change with the path of each file (Train, Test, Val)\n",
        "ruta = pathlib.Path('/content/drive/MyDrive/EderReact/ElderReact_train')\n",
        "paths = []\n",
        "for archivo in ruta.glob('*.wav'):\n",
        "  paths.append(archivo.name)\n",
        "  #print(archivo.name)"
      ],
      "metadata": {
        "id": "TEHkb0lh9Ndt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = pathlib.Path('/content/drive/MyDrive/EderReact/ElderReact_train')\n",
        "paths_txt = []\n",
        "for archivo in ruta.glob('*.txt'):\n",
        "  paths_txt.append(archivo.name)\n",
        "  #print(archivo.name)"
      ],
      "metadata": {
        "id": "xPuXQcbz9kMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear dataframes individuales para cada tipo de archivo\n",
        "df_wav = pd.DataFrame({'name': [archivo.split('.')[0] for archivo in paths], 'wav': paths})\n",
        "df_txt = pd.DataFrame({'name': [archivo.split('.')[0] for archivo in paths_txt], 'txt': paths_txt})\n",
        "\n",
        "\n",
        "# Unir los dataframes basados en la columna 'name'\n",
        "train_df = pd.merge(df_wav, df_txt, on='name', how='outer')\n",
        "train_df = pd.merge(train_df, df_tg, on='name', how='outer')\n",
        "\n",
        "\n",
        "train_df\n",
        "train_df.to_csv('train.csv', index=False)"
      ],
      "metadata": {
        "id": "4LnoTYGv-Dw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Map labels"
      ],
      "metadata": {
        "id": "fx4U9uy7-o7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "#1->filename, 2->Anger, 3->Disgust, 4->Fear, 5->Happiness, 6->Sadness, 7->Surprise, 8->Gender, 9->Valence).\n",
        "with open('/content/train_labels.txt', 'r') as in_file:\n",
        "    stripped = (line.strip() for line in in_file)\n",
        "    lines = (line.split(\" \") for line in stripped if line)\n",
        "    with open('log.csv', 'w') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        writer.writerow(('filename', 'Anger','Disgust','Fear','Happiness','Sadness','Surprise','Gender','Valence'))\n",
        "        writer.writerows(lines)"
      ],
      "metadata": {
        "id": "DLA_SnRl-1rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta train\n",
        "path_train = '/content/train.csv'\n",
        "df_files = pd.read_csv(path_train)\n",
        "# Ruta labels\n",
        "path_labels = '/content/log.csv'\n",
        "df_labels = pd.read_csv(path_labels)\n",
        "#Eliminar extensiones\n",
        "df_labels['name'] = df_labels['filename'].str.split('.').str[0]\n",
        "\n",
        "df_combined = pd.merge(df_files, df_labels, on='name', how='inner')\n",
        "\n",
        "df_combined.to_csv('train_labels.csv', index=False)"
      ],
      "metadata": {
        "id": "hhT2F__3_AKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Some visualizations**"
      ],
      "metadata": {
        "id": "uDPGVOq3_Akr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/EderReact/test_labels.csv')\n",
        "test"
      ],
      "metadata": {
        "id": "4sz-fHzcAnAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graficar las emociones de anger,disgust,fear,happiness,sadness,surprise\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "emociones = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
        "conteo = test[emociones].sum()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=conteo.index, y=conteo.values)\n",
        "plt.xlabel('Emociones')\n",
        "plt.ylabel('Conteo')"
      ],
      "metadata": {
        "id": "Dy7PmxgtAppI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.isnull().sum()"
      ],
      "metadata": {
        "id": "JX5-5YDOAuGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nosie visual and reduction"
      ],
      "metadata": {
        "id": "vVcVouOmAw1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install noisereduce"
      ],
      "metadata": {
        "id": "rXCkg_t9BX20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import dirname, join as pjoin\n",
        "from scipy.io import wavfile\n",
        "import scipy.io"
      ],
      "metadata": {
        "id": "M04zNEb7Baxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/EderReact/ElderReact_train'"
      ],
      "metadata": {
        "id": "LbX_ervkBcQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta = pathlib.Path(file_path)\n",
        "train = []\n",
        "for archivo in ruta.glob('*.wav'):\n",
        "  train.append(archivo)\n",
        "  #print(archivo.name)"
      ],
      "metadata": {
        "id": "s1QGm9HvBhg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "id": "NDqWtYx_BlL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import noisereduce as nr\n",
        "import numpy as np\n",
        "# load data\n",
        "rate, data = wavfile.read(train[0])\n",
        "orig_shape = data.shape\n",
        "# perform noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=data,\n",
        "                                sr=rate)"
      ],
      "metadata": {
        "id": "NJUjC_6kBnyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_noise"
      ],
      "metadata": {
        "id": "DX9UmGy6Buxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(data, color='gray')\n",
        "plt.title(\"Audio Original\")\n",
        "plt.xlabel(\"Muestras\")\n",
        "plt.ylabel(\"Amplitud\")\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(reduced_noise, color='purple')\n",
        "plt.title(\"Audio con Reducción de Ruido\")\n",
        "plt.xlabel(\"Muestras\")\n",
        "plt.ylabel(\"Amplitud\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wrfc4bKUBxvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar el audio\n",
        "file_path = train[0]  # Cambia esto por tu archivo\n",
        "audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "# Tomar una parte de silencio (primeros 0.5 segundos)\n",
        "silence_part = audio[:int(sr * 0.5)]\n",
        "\n",
        "# Graficar el ruido en el tiempo\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(silence_part, color='purple')\n",
        "plt.xlabel(\"Tiempo (muestras)\")\n",
        "plt.ylabel(\"Amplitud\")\n",
        "plt.title(\"Forma de Onda del Ruido\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WB13HzwxB1Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import noisereduce as nr\n",
        "import IPython.display as ipd\n",
        "\n",
        "file_path = train[0]\n",
        "audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "# Reducir ruido\n",
        "reduced_audio = nr.reduce_noise(y=audio, sr=sr)\n",
        "\n",
        "#  audio original\n",
        "print(\"🔊 Audio Original:\")\n",
        "ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "# audio con reducción de ruido\n",
        "print(\"🔊 Audio con Ruido Reducido:\")\n",
        "ipd.display(ipd.Audio(reduced_audio, rate=sr))"
      ],
      "metadata": {
        "id": "8yv7bxX7B30s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Average Noise"
      ],
      "metadata": {
        "id": "h7-9bSIhB5zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ruta donde están los archivos de audio\n",
        "audio_folder = \"/content/drive/MyDrive/EderReact/ElderReact_train\"\n",
        "\n",
        "# Lista de archivos en la carpeta\n",
        "audio_files = [f for f in os.listdir(audio_folder) if f.endswith(\".wav\")]\n",
        "\n",
        "# Lista para almacenar los niveles de ruido\n",
        "noise_levels = []\n",
        "\n",
        "for file in audio_files:\n",
        "    file_path = os.path.join(audio_folder, file)\n",
        "\n",
        "    # Cargar el audio\n",
        "    audio, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    # Tomar una parte silenciosa (los primeros 0.5 segundos)\n",
        "    silence_part = audio[:int(sr * 0.5)]\n",
        "\n",
        "    # Calcular el nivel de ruido (energía media del silencio)\n",
        "    noise_energy = np.mean(np.abs(silence_part))\n",
        "\n",
        "    # Guardar nivel de ruido del archivo\n",
        "    noise_levels.append(noise_energy)\n",
        "\n",
        "# Crear gráfico de barras\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(audio_files, noise_levels, color='purple')\n",
        "plt.xlabel(\"Archivos de Audio\")\n",
        "plt.ylabel(\"Nivel de Ruido\")\n",
        "plt.title(\"Ruido Promedio por Archivo de Audio\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nBBBRvITB_pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Duration of audios"
      ],
      "metadata": {
        "id": "pQ4PMQpKCB9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.boxplot(x=df_duraciones['Duración (s)'], color='mediumpurple')\n",
        "plt.title('Distribución de Duraciones de Audios')\n",
        "plt.xlabel('Duración (segundos)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6QMC1ZVdDTP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df_duraciones['Duración (s)'], bins=20, kde=True, color='indigo')\n",
        "plt.title('Histograma de Duraciones de Audios')\n",
        "plt.xlabel('Duración (segundos)')\n",
        "plt.ylabel('Cantidad de Audios')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_RFLgmecDU7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprpocessing"
      ],
      "metadata": {
        "id": "OjV9qVO9D8c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "map text"
      ],
      "metadata": {
        "id": "wVfwIF3oD-oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# archivo train\n",
        "test_path = pathlib.Path('/content/drive/MyDrive/EderReact/ElderReact_train')\n",
        "test_text = []\n",
        "for archivo in test_path.glob('*.txt'):\n",
        "  contenido = archivo.read_text()\n",
        "  test_text.append({'file': archivo.name, 'text': contenido})\n",
        "  #print(archivo.name)\n",
        "  #print(archivo.read_text())"
      ],
      "metadata": {
        "id": "mx-lCBxfEBO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = pd.DataFrame(test_text)\n",
        "test_text"
      ],
      "metadata": {
        "id": "sdnqAXw2EC95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_copy['text'] = test_text_copy[\"text\"].str.lower()\n",
        "test_text_copy.head()"
      ],
      "metadata": {
        "id": "BSA_68PeEEtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "KlMteEltEGmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "test_text_copy['text'] = test_text_copy['text'].apply(contractions.fix)\n",
        "test_text_copy['text']"
      ],
      "metadata": {
        "id": "dEOCAjBTEHhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"custom function to remove the punctuation\"\"\"\n",
        "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
        "\n",
        "test_text_copy[\"text\"] = test_text_copy[\"text\"].apply(lambda text: remove_punctuation(text))\n",
        "test_text_copy.head()"
      ],
      "metadata": {
        "id": "0qusurceEMWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "h5fI-BDmEO-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the correct package here.\n",
        "nltk.download('punkt_tab')\n",
        "text_lemm = []\n",
        "for text in test_text_copy['text']:\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Function to get the part of speech tag for lemmatization\n",
        "    def get_wordnet_pos(word):\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "    # Join the lemmatized words back into a single string\n",
        "    lemmatized_text = ' '.join(lemmatized_words)\n",
        "    text_lemm.append(lemmatized_text)\n",
        "    print(lemmatized_text)"
      ],
      "metadata": {
        "id": "-Ia1FeSCESyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text_copy['texto_lematizado'] = text_lemm\n",
        "test_text_copy"
      ],
      "metadata": {
        "id": "uMflHOv5EUfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = pd.read_csv('/content/drive/MyDrive/EderReact/train_labels.csv')\n",
        "test_labels"
      ],
      "metadata": {
        "id": "PkXAGtXrEX7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EMBEDDINGS**"
      ],
      "metadata": {
        "id": "SJo9W9HWEatu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre save all texts and their labels ina csv file"
      ],
      "metadata": {
        "id": "QRPOmeUOXEK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text embeddings"
      ],
      "metadata": {
        "id": "qklEWlCSEytf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "df = pd.read_csv('C:/Users/carod/Documents/todos.csv')\n",
        "\n",
        "# Lista para almacenar los embeddings\n",
        "embeddings_list = []\n",
        "\n",
        "# Iterar sobre cada texto del DataFrame\n",
        "for idx, row in df.iterrows():\n",
        "    text = row[\"text\"]  # Obtener el texto\n",
        "    name = row[\"file\"]  # Obtener el nombre del archivo\n",
        "\n",
        "    # Tokenización\n",
        "    encoding = tokenizer(text, padding=True, truncation=True, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    # Obtener los embeddings de la última capa\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoding)\n",
        "\n",
        "    # Extraer el embedding de la última capa (CLS token)\n",
        "    embedding = output.last_hidden_state[:, 0, :].squeeze().numpy()  # (1, 768) → (768,)\n",
        "\n",
        "    # Guardar en una lista\n",
        "    embeddings_list.append([name] + embedding.tolist())\n",
        "\n",
        "# Crear un DataFrame con los embeddings\n",
        "columns = [\"name\"] + [f\"dim_{i+1}\" for i in range(768)]\n",
        "embeddings_df = pd.DataFrame(embeddings_list, columns=columns)\n",
        "\n",
        "# Guardar en un CSV\n",
        "embeddings_df.to_csv(\"embeddings.csv\", index=False)\n",
        "\n",
        "print(\"Embeddings guardados en 'embeddings.csv'\")"
      ],
      "metadata": {
        "id": "niJHnH0UE9g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Embeddings"
      ],
      "metadata": {
        "id": "dYd0klEnXKmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import csv\n",
        "from os import listdir\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
        "\n",
        "# Ruta de los archivos de audio\n",
        "ruta1 = 'C:/Users/carod/Documents/Solo_mitadTrain'\n",
        "archivos = sorted([f for f in listdir(ruta1) if f.endswith('.wav')])\n",
        "\n",
        "# Cargar el modelo y el feature extractor una sola vez\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')\n",
        "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv')\n",
        "\n",
        "# Nombre del archivo CSV\n",
        "nombre_archivo = \"vectores_train2.csv\"\n",
        "\n",
        "# Definir tamaño del lote\n",
        "batch_size = 10\n",
        "num_batches = len(archivos) // batch_size + (1 if len(archivos) % batch_size != 0 else 0)\n",
        "\n",
        "# Escribir encabezado (nombre del archivo + 512 dimensiones)\n",
        "with open(nombre_archivo, 'w', newline='') as archivo:\n",
        "    escritor_csv = csv.writer(archivo)\n",
        "    escritor_csv.writerow([\"archivo\"] + [f\"dim_{i}\" for i in range(512)])\n",
        "\n",
        "# Procesar archivos en lotes\n",
        "for batch in range(num_batches):\n",
        "    start_idx = batch * batch_size\n",
        "    end_idx = min((batch + 1) * batch_size, len(archivos))\n",
        "\n",
        "    datos_concatenados = []\n",
        "    nombres_archivos = []  # Lista para guardar los nombres\n",
        "\n",
        "    print(f\"Procesando lote {batch + 1}/{num_batches}...\")\n",
        "\n",
        "    for a in archivos[start_idx:end_idx]:\n",
        "        try:\n",
        "            r = f\"{ruta1}/{a}\"\n",
        "            y, sr = librosa.load(r, sr=16000)\n",
        "\n",
        "            # Forzar una longitud estándar (ejemplo: 5 segundos = 80000 muestras)\n",
        "            max_length = 80000  # 5 segundos a 16kHz\n",
        "            if len(y) < max_length:\n",
        "                y = np.pad(y, (0, max_length - len(y)))  # Rellenar con ceros\n",
        "            else:\n",
        "                y = y[:max_length]  # Cortar si es más largo\n",
        "\n",
        "            datos_concatenados.append(y)\n",
        "            nombres_archivos.append(a)  # Guardar el nombre del archivo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error con archivo {a}: {e}\")\n",
        "\n",
        "    if not datos_concatenados:\n",
        "        continue  # Si no hay datos válidos, saltar el lote\n",
        "\n",
        "    # Convertir a tensores y obtener embeddings\n",
        "    datos_convertidos = [np.array(datos) for datos in datos_concatenados]\n",
        "    inputs = feature_extractor(datos_convertidos, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():  # Desactivar gradientes para ahorrar memoria\n",
        "        embeddings = model(**inputs).embeddings\n",
        "        embeddings1 = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n",
        "\n",
        "    vectores1 = embeddings1.tolist()\n",
        "\n",
        "    # Guardar los vectores en el archivo CSV con el nombre del archivo\n",
        "    with open(nombre_archivo, 'a', newline='') as archivo:\n",
        "        escritor_csv = csv.writer(archivo)\n",
        "        for nombre, vector in zip(nombres_archivos, vectores1):\n",
        "            escritor_csv.writerow([nombre] + vector)\n",
        "\n",
        "    # Liberar memoria\n",
        "    del datos_concatenados, nombres_archivos, datos_convertidos, inputs, embeddings, embeddings1\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Procesamiento completado.\")"
      ],
      "metadata": {
        "id": "4GePfajMXOUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Characteristics from video using Mediapipe Facemesh"
      ],
      "metadata": {
        "id": "U9PTBi2PXWMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "carpeta_train = \"C:/Users/carod/Documents/ElderReact_dev\"\n",
        "\n",
        "train_videos = list(pathlib.Path(carpeta_train).glob('*.mp4'))\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Inicializar MediaPipe FaceMesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "# Puntos de referencia\n",
        "EYEBROW_RIGHT = [143, 156, 70, 63, 105, 66, 107]\n",
        "EYEBROW_LEFT = [336, 296, 334, 293, 300, 383, 372]\n",
        "FOREHEAD = [10]\n",
        "\n",
        "UPPER_MOUTH = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308]\n",
        "LOWER_MOUTH = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
        "\n",
        "UPPER_EYELID_RIGHT = [159, 160, 161]\n",
        "LOWER_EYELID_RIGHT = [145, 144, 153]\n",
        "UPPER_EYELID_LEFT = [386, 385, 384]\n",
        "LOWER_EYELID_LEFT = [374, 380, 373]\n",
        "\n",
        "CHEEKS = [93, 323]  # Puntos de las mejillas\n",
        "\n",
        "# Función para calcular distancia euclidiana\n",
        "def euclidean_distance(pt1, pt2):\n",
        "    return np.linalg.norm(np.array(pt1) - np.array(pt2))\n",
        "\n",
        "# Lista con las rutas de los videos\n",
        "video_paths = train_videos\n",
        "\n",
        "# Ruta del CSV donde se guardarán los resultados\n",
        "output_csv = \"C:/Users/carod/Documents/resultadosVal.csv-\"\n",
        "\n",
        "# Procesar cada video en la lista\n",
        "for video_path in video_paths:\n",
        "    video_name = os.path.basename(video_path)  # Extraer el nombre del archivo\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    data = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        h, w, _ = frame.shape\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(rgb_frame)\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            for face_landmarks in results.multi_face_landmarks:\n",
        "                landmarks = [(int(pt.x * w), int(pt.y * h)) for pt in face_landmarks.landmark]\n",
        "\n",
        "                # Distancia de las cejas a la frente\n",
        "                forehead_right_distances = [euclidean_distance(landmarks[p], landmarks[FOREHEAD[0]]) for p in EYEBROW_RIGHT]\n",
        "                forehead_left_distances = [euclidean_distance(landmarks[p], landmarks[FOREHEAD[0]]) for p in EYEBROW_LEFT]\n",
        "\n",
        "                avg_forehead_right = np.mean(forehead_right_distances)\n",
        "                avg_forehead_left = np.mean(forehead_left_distances)\n",
        "\n",
        "                # Distancia de las mejillas al párpado inferior\n",
        "                cheek_right_distance = np.mean([euclidean_distance(landmarks[CHEEKS[0]], landmarks[p]) for p in LOWER_EYELID_RIGHT])\n",
        "                cheek_left_distance = np.mean([euclidean_distance(landmarks[CHEEKS[1]], landmarks[p]) for p in LOWER_EYELID_LEFT])\n",
        "\n",
        "                # Apertura de los ojos\n",
        "                eye_right_opening = np.mean([euclidean_distance(landmarks[u], landmarks[l]) for u, l in zip(UPPER_EYELID_RIGHT, LOWER_EYELID_RIGHT)])\n",
        "                eye_left_opening = np.mean([euclidean_distance(landmarks[u], landmarks[l]) for u, l in zip(UPPER_EYELID_LEFT, LOWER_EYELID_LEFT)])\n",
        "\n",
        "                # Apertura de la boca\n",
        "                mouth_opening = np.mean([euclidean_distance(landmarks[u], landmarks[l]) for u, l in zip(UPPER_MOUTH, LOWER_MOUTH)])\n",
        "\n",
        "                data.append([\n",
        "                    avg_forehead_right, avg_forehead_left, cheek_right_distance, cheek_left_distance,\n",
        "                    eye_right_opening, eye_left_opening, mouth_opening\n",
        "                ])\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Crear DataFrame con los datos del video\n",
        "    df = pd.DataFrame(data, columns=[\n",
        "        'forehead_right_distance', 'forehead_left_distance',\n",
        "        'cheek_right_distance', 'cheek_left_distance',\n",
        "        'eye_right_opening', 'eye_left_opening', 'mouth_opening'\n",
        "    ])\n",
        "\n",
        "    # Calcular la media de cada columna\n",
        "    if not df.empty:  # Verificar que el DataFrame no esté vacío\n",
        "        mean_vector = df.mean().to_frame().T  # Convertir a DataFrame de una sola fila\n",
        "        mean_vector['video_name'] = video_name  # Agregar el nombre del video como columna\n",
        "\n",
        "        # Guardar en CSV (si el archivo no existe, crea uno nuevo con encabezado, si existe, agrega datos)\n",
        "        if not os.path.exists(output_csv):\n",
        "            mean_vector.to_csv(output_csv, index=False)  # Escribir con encabezado\n",
        "        else:\n",
        "            mean_vector.to_csv(output_csv, mode='a', header=False, index=False)  # Agregar sin encabezado\n",
        "\n",
        "    print(f\"Procesamiento completado para {video_name}\")\n",
        "\n",
        "print(\"Todos los videos han sido procesados. Resultados guardados en\", output_csv)\n"
      ],
      "metadata": {
        "id": "whCS8tpsXaqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}